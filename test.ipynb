{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a63b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subspace_partition.subspace_partition import (\n",
    "    run_subspace_partition,\n",
    "    SubspacePartitionConfig,\n",
    ")\n",
    "\n",
    "import transformer_lens\n",
    "from pathlib import Path\n",
    "import copy_transformer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3cfdc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDNING_DIM = 64\n",
    "NUM_HEADS = 8\n",
    "VOCABULARY = [c for c in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"]\n",
    "CONTEXT_LENGTH = 32\n",
    "\n",
    "EXPERIMENT_NAME = \"test_experiment\"\n",
    "OUTPUT_DIR = Path(\"test_outputs\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46694251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class SimpleCharTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"Super simple character tokenizer that takes a list of chars as alphabet\"\"\"\n",
    "\n",
    "    def __init__(self, alphabet: List[str], **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alphabet: List of characters to use as vocabulary\n",
    "        \"\"\"\n",
    "        # Store alphabet\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "        # Create vocab mapping: char -> id\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(alphabet)}\n",
    "        self.id_to_char = {idx: char for char, idx in self.char_to_id.items()}\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Add special tokens to vocab after parent init\n",
    "        special_tokens = [\n",
    "            (self.bos_token, self.bos_token_id),\n",
    "            (self.eos_token, self.eos_token_id),\n",
    "            (self.unk_token, self.unk_token_id),\n",
    "            (self.pad_token, self.pad_token_id),\n",
    "        ]\n",
    "        for token, token_id in special_tokens:\n",
    "            if token and token_id is not None:\n",
    "                self.char_to_id[token] = token_id\n",
    "                self.id_to_char[token_id] = token\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.char_to_id)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.char_to_id.copy()\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into individual characters\"\"\"\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        \"\"\"Convert character to ID\"\"\"\n",
    "        return self.char_to_id.get(token, self.char_to_id.get(self.unk_token, 0))\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        \"\"\"Convert ID to character\"\"\"\n",
    "        return self.id_to_char.get(index, self.unk_token or \"\")\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
    "        \"\"\"Join characters back into string\"\"\"\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def save_vocabulary(\n",
    "        self, save_directory: str, filename_prefix: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Save vocabulary to file\"\"\"\n",
    "        import json\n",
    "        import os\n",
    "\n",
    "        if not os.path.isdir(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        vocab_file = os.path.join(\n",
    "            save_directory,\n",
    "            (filename_prefix + \"-\" if filename_prefix else \"\") + \"vocab.json\",\n",
    "        )\n",
    "\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.char_to_id, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return (vocab_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747df2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleCharTokenizer(\n",
    "    alphabet=VOCABULARY,\n",
    "    bos_token=\">\",\n",
    "    eos_token=\"<\",\n",
    "    unk_token=\"?\",\n",
    "    pad_token=\"_\",\n",
    "    name_or_path=\"custom\",\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_config = transformer_lens.HookedTransformerConfig(\n",
    "    d_model=EMBEDDNING_DIM,\n",
    "    d_head=EMBEDDNING_DIM // NUM_HEADS,\n",
    "    n_layers=2,\n",
    "    n_ctx=CONTEXT_LENGTH,\n",
    "    n_heads=NUM_HEADS,\n",
    "    d_vocab=tokenizer.vocab_size,\n",
    "    attn_only=True,\n",
    ")\n",
    "model_state_dict_path = Path(\"copy_transformer.pt\")\n",
    "\n",
    "subspace_partition_config = SubspacePartitionConfig(\n",
    "    exp_name=EXPERIMENT_NAME,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    model_config=model_config,\n",
    "    model_weights_path=model_state_dict_path,\n",
    "    act_sites=[\"blocks.0.hook_resid_post\", \"blocks.1.hook_resid_post\"],\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=copy_transformer.data.IterablePureRepeatingPatternDataset(\n",
    "        num_samples=10_000,\n",
    "        vocabulary=VOCABULARY,\n",
    "        context_length=CONTEXT_LENGTH,\n",
    "        max_pattern_length=7,\n",
    "    ),\n",
    "    max_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3283244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for blocks.0.hook_resid_post\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juliusk/_/Uni/Kurse_Winter_2025_2026/AI Safety Incubator/Research_Project/SubspacePartition/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      " 40%|████      | 201/500 [00:32<00:46,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R_grad_norm': 0.015745067843236028, 'training_loss': 0.10712239142507314}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 401/500 [01:03<00:16,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R_grad_norm': 0.016980729922652246, 'training_loss': 0.10401578504592181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:19<00:00,  6.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish training (500)\n",
      "saving.. test_outputs/test_experiment\n",
      "evaluating (25 steps)...\n",
      " ******* eval result *******\n",
      "mean (weighted) 0.1004851758480072\n",
      "mean (unweighted) 0.1004851758480072\n",
      "tensor([0.0958, 0.1052])\n",
      "training for blocks.1.hook_resid_post\n",
      " ******* eval result *******\n",
      "mean (weighted) 0.1004851758480072\n",
      "mean (unweighted) 0.1004851758480072\n",
      "tensor([0.0958, 0.1052])\n",
      "training for blocks.1.hook_resid_post\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201/500 [00:32<00:50,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R_grad_norm': 0.04853655453771353, 'training_loss': 0.34381822042167187}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 401/500 [01:07<00:16,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R_grad_norm': 0.07641626667231322, 'training_loss': 0.331877730935812}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:23<00:00,  5.98it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish training (500)\n",
      "saving.. test_outputs/test_experiment\n",
      "evaluating (25 steps)...\n",
      " ******* eval result *******\n",
      "mean (weighted) 0.32304659485816956\n",
      "mean (unweighted) 0.32304659485816956\n",
      "tensor([0.2947, 0.3514])\n",
      " ******* eval result *******\n",
      "mean (weighted) 0.32304659485816956\n",
      "mean (unweighted) 0.32304659485816956\n",
      "tensor([0.2947, 0.3514])\n"
     ]
    }
   ],
   "source": [
    "run_subspace_partition(subspace_partition_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subspacepartition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
