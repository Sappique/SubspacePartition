{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a09625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy_transformer.data\n",
    "import copy_transformer.tokenizer\n",
    "import copy_transformer.training\n",
    "\n",
    "import torch\n",
    "import transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7975e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDNING_DIM = 64\n",
    "NUM_HEADS = 8\n",
    "VOCABULARY = [c for c in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"]\n",
    "CONTEXT_LENGTH = 32\n",
    "\n",
    "NUM_SAMPLES = 100_000\n",
    "MAX_PATTERN_LENGTH = 16\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import List, Optional\n",
    "\n",
    "class SimpleCharTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"Super simple character tokenizer that takes a list of chars as alphabet\"\"\"\n",
    "    \n",
    "    def __init__(self, alphabet: List[str], **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alphabet: List of characters to use as vocabulary\n",
    "        \"\"\"\n",
    "        # Store alphabet\n",
    "        self.alphabet = alphabet\n",
    "        \n",
    "        # Create vocab mapping: char -> id\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(alphabet)}\n",
    "        self.id_to_char = {idx: char for char, idx in self.char_to_id.items()}\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Add special tokens to vocab after parent init\n",
    "        special_tokens = [\n",
    "            (self.bos_token, self.bos_token_id),\n",
    "            (self.eos_token, self.eos_token_id),\n",
    "            (self.unk_token, self.unk_token_id),\n",
    "            (self.pad_token, self.pad_token_id),\n",
    "        ]\n",
    "        for token, token_id in special_tokens:\n",
    "            if token and token_id is not None:\n",
    "                self.char_to_id[token] = token_id\n",
    "                self.id_to_char[token_id] = token\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.char_to_id)\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.char_to_id.copy()\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into individual characters\"\"\"\n",
    "        return list(text)\n",
    "    \n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        \"\"\"Convert character to ID\"\"\"\n",
    "        return self.char_to_id.get(token, self.char_to_id.get(self.unk_token, 0))\n",
    "    \n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        \"\"\"Convert ID to character\"\"\"\n",
    "        return self.id_to_char.get(index, self.unk_token or \"\")\n",
    "    \n",
    "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
    "        \"\"\"Join characters back into string\"\"\"\n",
    "        return \"\".join(tokens)\n",
    "    \n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None):\n",
    "        \"\"\"Save vocabulary to file\"\"\"\n",
    "        import json\n",
    "        import os\n",
    "        \n",
    "        if not os.path.isdir(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        vocab_file = os.path.join(\n",
    "            save_directory, \n",
    "            (filename_prefix + \"-\" if filename_prefix else \"\") + \"vocab.json\"\n",
    "        )\n",
    "        \n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.char_to_id, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        return (vocab_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = transformer_lens.HookedTransformerConfig(\n",
    "    d_model=EMBEDDNING_DIM,\n",
    "    d_head=EMBEDDNING_DIM // NUM_HEADS,\n",
    "    n_layers=2,\n",
    "    n_ctx=CONTEXT_LENGTH,\n",
    "    n_heads=NUM_HEADS,\n",
    "    d_vocab=VOCABULARY_SIZE,\n",
    "    attn_only=True,\n",
    ")\n",
    "model = transformer_lens.HookedTransformer(model_config)\n",
    "tokenizer = SimpleCharTokenizer(alphabet=VOCABULARY,\n",
    "                                 bos_token=\">\",\n",
    "                                 eos_token=\"<\",\n",
    "                                 unk_token=\"?\",\n",
    "                                 pad_token=\"_\")\n",
    "dataset = copy_transformer.data.PureRepeatingPatternDataset(\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    vocabulary=VOCABULARY,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    max_pattern_length=MAX_PATTERN_LENGTH,\n",
    ")\n",
    "training_set, validation_set = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set, batch_size=BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c52e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Loss: 0.9237\n",
      "Epoch 2/10, Validation Loss: 0.8997\n",
      "Epoch 3/10, Validation Loss: 0.8832\n",
      "Epoch 4/10, Validation Loss: 0.8792\n",
      "Epoch 5/10, Validation Loss: 0.8782\n",
      "Epoch 6/10, Validation Loss: 0.8762\n",
      "Epoch 7/10, Validation Loss: 0.8743\n",
      "Epoch 8/10, Validation Loss: 0.8743\n",
      "Epoch 9/10, Validation Loss: 0.8730\n",
      "Epoch 10/10, Validation Loss: 0.8724\n"
     ]
    }
   ],
   "source": [
    "copy_transformer.training.train_transformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    training_loader=training_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49c1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"out/copy_transformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae4670cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ABCDEABCDEABCDEAB\"\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt)\n",
    "output = model(torch.tensor(tokenized_prompt).unsqueeze(0))\n",
    "next_token_prediction = output.squeeze()[-1].argmax().item()\n",
    "\n",
    "print(tokenizer.decode([next_token_prediction]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subspacepartition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
